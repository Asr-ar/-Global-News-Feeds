{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3d9b30",
   "metadata": {},
   "source": [
    "In this notebook \n",
    "\n",
    "- Text Preprocessing\n",
    " \n",
    "- Create the Document-Term Matrix and TF-IDF\n",
    "   \n",
    "- Apply Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf5f7d",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3274b51",
   "metadata": {},
   "source": [
    "Data manipulation libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a842ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f35a5",
   "metadata": {},
   "source": [
    "Visualization libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d232357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot  as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f25039",
   "metadata": {},
   "source": [
    "Pre-processing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98263967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as st\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf53f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text  import CountVectorizer , TfidfVectorizer\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "from nltk                             import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bccb014",
   "metadata": {},
   "source": [
    "Classes instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b97159e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "cv         = CountVectorizer(stop_words='english', max_df=5)\n",
    "cv_tfidf   = TfidfVectorizer(stop_words='english', max_df=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649b301",
   "metadata": {},
   "source": [
    "### Data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0ba061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookproretina13inch/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "file_path = 'English_Global_news.csv'\n",
    "\n",
    "English_Global_news = pd.read_csv(file_path , index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353cdcf",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffd4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1263809 entries, 0 to 3327276\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   Text      1263809 non-null  object\n",
      " 1   language  1263809 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 28.9+ MB\n"
     ]
    }
   ],
   "source": [
    "English_Global_news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d45fb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here Are the Details on Facebook's Global Part...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Petrol &amp; diesel on the rise post daily price r...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text language\n",
       "0  Here Are the Details on Facebook's Global Part...  English\n",
       "3  Petrol & diesel on the rise post daily price r...  English"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13856df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>450228</th>\n",
       "      <td>Fin24.com - Gigaba and CEOs air concerns over ...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152785</th>\n",
       "      <td>Sarah Harding takes swipe at 'savvy' Cheryl ov...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text language\n",
       "450228  Fin24.com - Gigaba and CEOs air concerns over ...  English\n",
       "152785  Sarah Harding takes swipe at 'savvy' Cheryl ov...  English"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c6ce73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3327272</th>\n",
       "      <td>armed forces may be organized as standing forc...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327276</th>\n",
       "      <td>the total high school population was now appro...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Text language\n",
       "3327272  armed forces may be organized as standing forc...  English\n",
       "3327276  the total high school population was now appro...  English"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8d163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 1263809 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print('Data has {} rows and {} columns'.format(English_Global_news.shape[0], English_Global_news.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a67e9",
   "metadata": {},
   "source": [
    "### Checking for NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9542218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b0816",
   "metadata": {},
   "source": [
    "### Droping any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "151c667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "English_Global_news = English_Global_news.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69302bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing (Nans - duplicates) the data has 867786 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print('After removing (Nans - duplicates) the data has {} rows and {} columns'\n",
    "      .format(English_Global_news.shape[0], English_Global_news.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147c9ca",
   "metadata": {},
   "source": [
    "### Remove the unneeded column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "295255c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "English_Global_news = English_Global_news.drop('language', axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acdf69b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(867786, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127aa5a",
   "metadata": {},
   "source": [
    "###  Taking a sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1b27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "English_Global_news_sample = English_Global_news[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4a35939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad3d50",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a54cd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precosseing_pipeline(text):\n",
    "        # remove urls\n",
    "        text = re.sub(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', ' ', text)\n",
    "        # remove punctuations \n",
    "        text = (\"\".join([ch for ch in text if ch not in st.punctuation]))\n",
    "        # remove non-alphanumeric characters\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "        # lower casing\n",
    "        text = text.lower()\n",
    "        # convert text to tokens\n",
    "        text = re.split('\\s+' ,text)\n",
    "        tokens = [x.lower() for x in text]\n",
    "        # remove stopwords using NLTK corpus stopwords list to match\n",
    "        tokens = [word for word in text if word not in nltk.corpus.stopwords.words('english')]\n",
    "        # convert words to feature vectors\n",
    "        text = \" \".join([word for word in tokens])     \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52a45159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-c706c6e8922b>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  English_Global_news_sample['Text'] = English_Global_news_sample['Text'].apply(precosseing_pipeline)\n"
     ]
    }
   ],
   "source": [
    "English_Global_news_sample['Text'] = English_Global_news_sample['Text'].apply(precosseing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b360d303",
   "metadata": {},
   "source": [
    "### Apply Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ec5c9",
   "metadata": {},
   "source": [
    "- Lemmatization: cut word down to base form using vocabulary and    morphological analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "318fac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatize(text):\n",
    "    text_split = text.split(' ')\n",
    "    lem_v_text = ''\n",
    "    \n",
    "    for text in text_split:\n",
    "        lem_v_text += lemmatizer.lemmatize(text, pos='v') + ' '\n",
    "        text_split  = lem_v_text.split(' ')\n",
    "        lem_text    =''\n",
    "        \n",
    "    for text in text_split:\n",
    "        lem_text += lemmatizer.lemmatize(text, pos='a') + ' '\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f2bb9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-183eb68da1a9>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  English_Global_news_sample['Text_lemma'] = English_Global_news_sample['Text'].apply(apply_lemmatize)\n"
     ]
    }
   ],
   "source": [
    "English_Global_news_sample['Text_lemma'] = English_Global_news_sample['Text'].apply(apply_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf88f3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23865</th>\n",
       "      <td>mcgregor promises break old man final press co...</td>\n",
       "      <td>mcgregor promise break old man final press con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6779</th>\n",
       "      <td>six democrats could beat trump including one t...</td>\n",
       "      <td>six democrats could beat trump include one tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19445</th>\n",
       "      <td>sirona biochem receives tsx venture exchange a...</td>\n",
       "      <td>sirona biochem receive tsx venture exchange ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32115</th>\n",
       "      <td>lottery hidden secret billions unclaimed prize...</td>\n",
       "      <td>lottery hide secret billions unclaimed prize p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24287</th>\n",
       "      <td>bomber oozing confidence</td>\n",
       "      <td>bomber ooze confidence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "23865  mcgregor promises break old man final press co...   \n",
       "6779   six democrats could beat trump including one t...   \n",
       "19445  sirona biochem receives tsx venture exchange a...   \n",
       "32115  lottery hidden secret billions unclaimed prize...   \n",
       "24287                           bomber oozing confidence   \n",
       "\n",
       "                                              Text_lemma  \n",
       "23865  mcgregor promise break old man final press con...  \n",
       "6779   six democrats could beat trump include one tex...  \n",
       "19445  sirona biochem receive tsx venture exchange ap...  \n",
       "32115  lottery hide secret billions unclaimed prize p...  \n",
       "24287                           bomber ooze confidence    "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news_sample.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149d72d",
   "metadata": {},
   "source": [
    "### Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8385c",
   "metadata": {},
   "source": [
    "- Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72c98005",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = English_Global_news_sample.Text_lemma\n",
    "X_cv =cv.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca5c3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Document_TM = pd.DataFrame(X_cv.toarray(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc1b4a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaco</th>\n",
       "      <th>aadhaar</th>\n",
       "      <th>aafias</th>\n",
       "      <th>aamc</th>\n",
       "      <th>aampw</th>\n",
       "      <th>aap</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aar</th>\n",
       "      <th>...</th>\n",
       "      <th>zosano</th>\n",
       "      <th>zska</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zumas</th>\n",
       "      <th>zumbrunwall</th>\n",
       "      <th>zwave</th>\n",
       "      <th>zweig</th>\n",
       "      <th>zyme</th>\n",
       "      <th>zzyzx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaco  aadhaar  aafias  aamc  aampw  aap  aapl  aar  ...  zosano  \\\n",
       "0   0    0     0        0       0     0      0    0     0    0  ...       0   \n",
       "1   0    0     0        0       0     0      0    0     0    0  ...       0   \n",
       "2   0    0     0        0       0     0      0    0     0    0  ...       0   \n",
       "3   0    0     0        0       0     0      0    0     0    0  ...       0   \n",
       "4   0    0     0        0       0     0      0    0     0    0  ...       0   \n",
       "\n",
       "   zska  zucchini  zuckerberg  zumas  zumbrunwall  zwave  zweig  zyme  zzyzx  \n",
       "0     0         0           0      0            0      0      0     0      0  \n",
       "1     0         0           0      0            0      0      0     0      0  \n",
       "2     0         0           0      0            0      0      0     0      0  \n",
       "3     0         0           0      0            0      0      0     0      0  \n",
       "4     0         0           0      0            0      0      0     0      0  \n",
       "\n",
       "[5 rows x 16797 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document_TM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8553017",
   "metadata": {},
   "source": [
    "#### Check cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a83fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity,pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a982ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(Document_TM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9aabd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "        1.00000000e+00, 2.22044605e-16, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.22044605e-16]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_distances(Document_TM,metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fadb4a",
   "metadata": {},
   "source": [
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20683812",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf  = cv_tfidf.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23b1b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF   = pd.DataFrame(X_tfidf, columns = cv_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b172726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaco</th>\n",
       "      <th>aadhaar</th>\n",
       "      <th>aafias</th>\n",
       "      <th>aamc</th>\n",
       "      <th>aampw</th>\n",
       "      <th>aap</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aar</th>\n",
       "      <th>...</th>\n",
       "      <th>zosano</th>\n",
       "      <th>zska</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zumas</th>\n",
       "      <th>zumbrunwall</th>\n",
       "      <th>zwave</th>\n",
       "      <th>zweig</th>\n",
       "      <th>zyme</th>\n",
       "      <th>zzyzx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 16797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aaa  aaco  aadhaar  aafias  aamc  aampw  aap  aapl  aar  ...  \\\n",
       "0      0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "1      0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "2      0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "3      0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "4      0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "...    ...  ...   ...      ...     ...   ...    ...  ...   ...  ...  ...   \n",
       "14995  0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "14996  0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "14997  0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "14998  0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "14999  0.0  0.0   0.0      0.0     0.0   0.0    0.0  0.0   0.0  0.0  ...   \n",
       "\n",
       "       zosano  zska  zucchini  zuckerberg  zumas  zumbrunwall  zwave  zweig  \\\n",
       "0         0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "1         0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "2         0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "3         0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "4         0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "...       ...   ...       ...         ...    ...          ...    ...    ...   \n",
       "14995     0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "14996     0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "14997     0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "14998     0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "14999     0.0   0.0       0.0         0.0    0.0          0.0    0.0    0.0   \n",
       "\n",
       "       zyme  zzyzx  \n",
       "0       0.0    0.0  \n",
       "1       0.0    0.0  \n",
       "2       0.0    0.0  \n",
       "3       0.0    0.0  \n",
       "4       0.0    0.0  \n",
       "...     ...    ...  \n",
       "14995   0.0    0.0  \n",
       "14996   0.0    0.0  \n",
       "14997   0.0    0.0  \n",
       "14998   0.0    0.0  \n",
       "14999   0.0    0.0  \n",
       "\n",
       "[15000 rows x 16797 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c629849",
   "metadata": {},
   "source": [
    "#### Check cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f7036b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01543a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 0., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 0., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_distances(TF_IDF,metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c540c02",
   "metadata": {},
   "source": [
    "###  Apply SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fabe2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b798f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-3c3cd880c561>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  English_Global_news_sample['spacy_doc'] = English_Global_news_sample['Text'].apply(nlp)\n"
     ]
    }
   ],
   "source": [
    "English_Global_news_sample['spacy_doc'] = English_Global_news_sample['Text'].apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e860768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_lemma</th>\n",
       "      <th>spacy_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>details facebooks global partner summit</td>\n",
       "      <td>detail facebooks global partner summit</td>\n",
       "      <td>(details, facebooks, global, partner, summit)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petrol diesel rise post daily price revisions ...</td>\n",
       "      <td>petrol diesel rise post daily price revisions ...</td>\n",
       "      <td>(petrol, diesel, rise, post, daily, price, rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>could deshone kizer end browns history qb misf...</td>\n",
       "      <td>could deshone kizer end brown history qb misfo...</td>\n",
       "      <td>(could, deshone, kizer, end, browns, history, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>comment microsoft never sneakily force windows...</td>\n",
       "      <td>comment microsoft never sneakily force windows...</td>\n",
       "      <td>(comment, microsoft, never, sneakily, force, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>comment google chrome enterprise techfan</td>\n",
       "      <td>comment google chrome enterprise techfan</td>\n",
       "      <td>(comment, google, chrome, enterprise, techfan)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0            details facebooks global partner summit   \n",
       "3  petrol diesel rise post daily price revisions ...   \n",
       "4  could deshone kizer end browns history qb misf...   \n",
       "5  comment microsoft never sneakily force windows...   \n",
       "6           comment google chrome enterprise techfan   \n",
       "\n",
       "                                          Text_lemma  \\\n",
       "0           detail facebooks global partner summit     \n",
       "3  petrol diesel rise post daily price revisions ...   \n",
       "4  could deshone kizer end brown history qb misfo...   \n",
       "5  comment microsoft never sneakily force windows...   \n",
       "6         comment google chrome enterprise techfan     \n",
       "\n",
       "                                           spacy_doc  \n",
       "0      (details, facebooks, global, partner, summit)  \n",
       "3  (petrol, diesel, rise, post, daily, price, rev...  \n",
       "4  (could, deshone, kizer, end, browns, history, ...  \n",
       "5  (comment, microsoft, never, sneakily, force, w...  \n",
       "6     (comment, google, chrome, enterprise, techfan)  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d590abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-e15b009280cb>:6: DeprecationWarning: [W107] The property `Doc.is_parsed` is deprecated. Use `Doc.has_annotation(\"DEP\")` instead.\n",
      "  if doc.is_parsed:\n",
      "<ipython-input-38-e15b009280cb>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  English_Global_news_sample['species_tokens'] = tokens\n",
      "<ipython-input-38-e15b009280cb>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  English_Global_news_sample['species_lemma']  = lemma\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "\n",
    "for doc in nlp.pipe(English_Global_news_sample['Text'].astype('unicode').values, batch_size=50,\n",
    "                       ):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        \n",
    "\n",
    "English_Global_news_sample['species_tokens'] = tokens\n",
    "English_Global_news_sample['species_lemma']  = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa3fd8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_lemma</th>\n",
       "      <th>spacy_doc</th>\n",
       "      <th>species_tokens</th>\n",
       "      <th>species_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>wdrb heine brothers raise kentucky science cen...</td>\n",
       "      <td>wdrb heine brothers raise kentucky science cen...</td>\n",
       "      <td>(wdrb, heine, brothers, raise, kentucky, scien...</td>\n",
       "      <td>[wdrb, heine, brothers, raise, kentucky, scien...</td>\n",
       "      <td>[wdrb, heine, brother, raise, kentucky, scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21624</th>\n",
       "      <td>honour uni getting students help</td>\n",
       "      <td>honour uni get students help</td>\n",
       "      <td>(honour, uni, getting, students, help)</td>\n",
       "      <td>[honour, uni, getting, students, help]</td>\n",
       "      <td>[honour, uni, get, student, help]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "353    wdrb heine brothers raise kentucky science cen...   \n",
       "21624                   honour uni getting students help   \n",
       "\n",
       "                                              Text_lemma  \\\n",
       "353    wdrb heine brothers raise kentucky science cen...   \n",
       "21624                     honour uni get students help     \n",
       "\n",
       "                                               spacy_doc  \\\n",
       "353    (wdrb, heine, brothers, raise, kentucky, scien...   \n",
       "21624             (honour, uni, getting, students, help)   \n",
       "\n",
       "                                          species_tokens  \\\n",
       "353    [wdrb, heine, brothers, raise, kentucky, scien...   \n",
       "21624             [honour, uni, getting, students, help]   \n",
       "\n",
       "                                           species_lemma  \n",
       "353    [wdrb, heine, brother, raise, kentucky, scienc...  \n",
       "21624                  [honour, uni, get, student, help]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news_sample.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eda0f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "404cb685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-3cb6423774c1>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  English_Global_news_sample['species_lemma'] = English_Global_news_sample['species_lemma'].astype(str).apply(get_text)\n"
     ]
    }
   ],
   "source": [
    "English_Global_news_sample['species_lemma'] = English_Global_news_sample['species_lemma'].astype(str).apply(get_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc7f015d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_lemma</th>\n",
       "      <th>spacy_doc</th>\n",
       "      <th>species_tokens</th>\n",
       "      <th>species_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23868</th>\n",
       "      <td>settlement means cash calls promising free cru...</td>\n",
       "      <td>settlement mean cash call promise free cruise</td>\n",
       "      <td>(settlement, means, cash, calls, promising, fr...</td>\n",
       "      <td>[settlement, means, cash, calls, promising, fr...</td>\n",
       "      <td>settlement    mean    cash    call    promis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27750</th>\n",
       "      <td>americans wish luck million powerball lottery ...</td>\n",
       "      <td>americans wish luck million powerball lottery ...</td>\n",
       "      <td>(americans, wish, luck, million, powerball, lo...</td>\n",
       "      <td>[americans, wish, luck, million, powerball, lo...</td>\n",
       "      <td>americans    wish    luck    million    powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34886</th>\n",
       "      <td>josh harrison spoils rich hills nohit bid thin...</td>\n",
       "      <td>josh harrison spoil rich hill nohit bid thin w...</td>\n",
       "      <td>(josh, harrison, spoils, rich, hills, nohit, b...</td>\n",
       "      <td>[josh, harrison, spoils, rich, hills, nohit, b...</td>\n",
       "      <td>josh    harrison    spoil    rich    hill   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>grenfell pm saves face admitting council flaws</td>\n",
       "      <td>grenfell pm save face admit council flaw</td>\n",
       "      <td>(grenfell, pm, saves, face, admitting, council...</td>\n",
       "      <td>[grenfell, pm, saves, face, admitting, council...</td>\n",
       "      <td>grenfell    pm    save    face    admit    c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13128</th>\n",
       "      <td>deciphering sweet satisfaction corn</td>\n",
       "      <td>decipher sweet satisfaction corn</td>\n",
       "      <td>(deciphering, sweet, satisfaction, corn)</td>\n",
       "      <td>[deciphering, sweet, satisfaction, corn]</td>\n",
       "      <td>decipher    sweet    satisfaction    corn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "23868  settlement means cash calls promising free cru...   \n",
       "27750  americans wish luck million powerball lottery ...   \n",
       "34886  josh harrison spoils rich hills nohit bid thin...   \n",
       "24980     grenfell pm saves face admitting council flaws   \n",
       "13128                deciphering sweet satisfaction corn   \n",
       "\n",
       "                                              Text_lemma  \\\n",
       "23868    settlement mean cash call promise free cruise     \n",
       "27750  americans wish luck million powerball lottery ...   \n",
       "34886  josh harrison spoil rich hill nohit bid thin w...   \n",
       "24980         grenfell pm save face admit council flaw     \n",
       "13128                 decipher sweet satisfaction corn     \n",
       "\n",
       "                                               spacy_doc  \\\n",
       "23868  (settlement, means, cash, calls, promising, fr...   \n",
       "27750  (americans, wish, luck, million, powerball, lo...   \n",
       "34886  (josh, harrison, spoils, rich, hills, nohit, b...   \n",
       "24980  (grenfell, pm, saves, face, admitting, council...   \n",
       "13128           (deciphering, sweet, satisfaction, corn)   \n",
       "\n",
       "                                          species_tokens  \\\n",
       "23868  [settlement, means, cash, calls, promising, fr...   \n",
       "27750  [americans, wish, luck, million, powerball, lo...   \n",
       "34886  [josh, harrison, spoils, rich, hills, nohit, b...   \n",
       "24980  [grenfell, pm, saves, face, admitting, council...   \n",
       "13128           [deciphering, sweet, satisfaction, corn]   \n",
       "\n",
       "                                           species_lemma  \n",
       "23868    settlement    mean    cash    call    promis...  \n",
       "27750    americans    wish    luck    million    powe...  \n",
       "34886    josh    harrison    spoil    rich    hill   ...  \n",
       "24980    grenfell    pm    save    face    admit    c...  \n",
       "13128        decipher    sweet    satisfaction    corn    "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "English_Global_news_sample.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e814c",
   "metadata": {},
   "source": [
    "### Store the DFs as CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87168a86",
   "metadata": {},
   "source": [
    "- Document_TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b88459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Document_TM.to_csv('Document_TM.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ab2ea",
   "metadata": {},
   "source": [
    "- TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd7ef446",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF.to_csv('TF_IDF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e47e6",
   "metadata": {},
   "source": [
    "- English_Global_news_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60aaf7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "English_Global_news_sample.to_csv('Sample.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
